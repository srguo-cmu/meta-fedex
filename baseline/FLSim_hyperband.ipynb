{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddffe6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: scikit-optimize in /opt/conda/lib/python3.8/site-packages (0.9.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.8/site-packages (from scikit-optimize) (1.1.0)\n",
      "Requirement already satisfied: pyaml>=16.9 in /opt/conda/lib/python3.8/site-packages (from scikit-optimize) (21.10.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /opt/conda/lib/python3.8/site-packages (from scikit-optimize) (0.24.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.8/site-packages (from scikit-optimize) (1.6.3)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.8/site-packages (from scikit-optimize) (1.21.4)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.8/site-packages (from pyaml>=16.9->scikit-optimize) (6.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn>=0.20.0->scikit-optimize) (3.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec66b82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperband_petro import hyperband\n",
    "from skopt.space import Real, Integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af8a1a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/leaf/data/sent140\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Most of the cells below are just taken from the sent_140 tutorial, for data loading\n",
    "'''\n",
    "\n",
    "# Don't need to run this after the directory has been created once\n",
    "\n",
    "'''\n",
    "\n",
    "%%capture preprocess_output\n",
    "\n",
    "# Download and preprocess the data\n",
    "% cd ..\n",
    "!git clone https://github.com/TalwalkarLab/leaf.git\n",
    "%cd leaf/data/sent140\n",
    "# !./preprocess.sh --sf 0.01 -s niid -t 'user' --tf 0.90 -k 1 --spltseed 1\n",
    "\n",
    "# change preprocess option (-t) so each user's data gets split into train/test\n",
    "!./preprocess.sh --sf 0.01 -s niid -t 'sample' --tf 0.90 -k 2 --spltseed 1\n",
    "\n",
    "'''\n",
    "\n",
    "%cd ../leaf/data/sent140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23cdb02c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data/train/all_data_niid_01_keep_2_train_9.json',\n",
       " 'data/test/all_data_niid_01_keep_2_test_9.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USE_CUDA = True\n",
    "LOCAL_BATCH_SIZE = 32\n",
    "MAX_SEQ_LEN = 25\n",
    "\n",
    "# suppress large outputs\n",
    "VERBOSE = False\n",
    "\n",
    "TRAIN_DATA = !ls data/train\n",
    "TRAIN_DATA = \"data/train/\" + TRAIN_DATA[0]\n",
    "\n",
    "TEST_DATA = !ls data/test\n",
    "TEST_DATA = \"data/test/\" + TEST_DATA[0]\n",
    "\n",
    "TRAIN_DATA, TEST_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc859c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of samples per user:\n",
      "  min=1, \n",
      "  max=123, \n",
      "  median=2.0, \n",
      "  mean=3.64, \n",
      "  std=6.37\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# load the training data\n",
    "with open(TRAIN_DATA, \"r\") as f:\n",
    "    training_data = json.load(f)\n",
    "\n",
    "# how samples are distributed across users\n",
    "n_samples = training_data['num_samples']\n",
    "print(f\"\"\"\\nNumber of samples per user:\n",
    "  min={np.min(n_samples)}, \n",
    "  max={np.max(n_samples)}, \n",
    "  median={np.median(n_samples)}, \n",
    "  mean={np.mean(n_samples):.2f}, \n",
    "  std={np.std(n_samples):.2f}\n",
    "  \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a0642f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': [['2003370575',\n",
       "   'Tue Jun 02 06:24:38 PDT 2009',\n",
       "   'NO_QUERY',\n",
       "   'bricaligirl',\n",
       "   'Doin my hair for school... Sooo tired ',\n",
       "   'training']],\n",
       " 'y': [0]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXAMPLE_USER = training_data[\"users\"][0]\n",
    "training_data[\"user_data\"][EXAMPLE_USER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "206ae3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "# 1. The Sent140Dataset will store the tweets and corresponding sentiment for each user.\n",
    "\n",
    "class Sent140Dataset(Dataset):\n",
    "    def __init__(self, data_root, max_seq_len):\n",
    "        self.data_root = data_root\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.all_letters = {c: i for i, c in enumerate(string.printable)}\n",
    "        self.num_letters = len(self.all_letters)\n",
    "        self.UNK = self.num_letters\n",
    "\n",
    "        with open(data_root, \"r+\") as f:\n",
    "            self.dataset = json.load(f)\n",
    "\n",
    "        self.data = {}\n",
    "        self.targets = {}\n",
    "        self.num_classes = 2  # binary sentiment classification\n",
    "\n",
    "        # Populate self.data and self.targets\n",
    "        for user_id, user_data in self.dataset[\"user_data\"].items():\n",
    "            self.data[user_id] = self.process_x(list(user_data[\"x\"]))\n",
    "            self.targets[user_id] = self.process_y(list(user_data[\"y\"]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for user_id in self.data.keys():\n",
    "            yield self.__getitem__(user_id)\n",
    "\n",
    "    def __getitem__(self, user_id: str):\n",
    "        if user_id not in self.data or user_id not in self.targets:\n",
    "            raise IndexError(f\"User {user_id} is not in dataset\")\n",
    "        return self.data[user_id], self.targets[user_id]\n",
    "\n",
    "    def unicodeToAscii(self, s):\n",
    "        return \"\".join(\n",
    "            c for c in unicodedata.normalize(\"NFD\", s)\n",
    "            if unicodedata.category(c) != \"Mn\" and c in self.all_letters\n",
    "        )\n",
    "\n",
    "    def line_to_indices(self, line: str, max_seq_len: int):\n",
    "        line_list = self.split_line(line)  # split phrase in words\n",
    "        line_list = line_list\n",
    "        chars = self.flatten_list([list(word) for word in line_list])\n",
    "        indices = [\n",
    "            self.all_letters.get(letter, self.UNK)\n",
    "            for i, letter in enumerate(chars)\n",
    "            if i < max_seq_len\n",
    "        ]\n",
    "        # Add padding\n",
    "        indices = indices + [self.UNK] * (max_seq_len - len(indices))\n",
    "        return indices\n",
    "\n",
    "    def process_x(self, raw_x_batch):\n",
    "        x_batch = [e[4] for e in raw_x_batch]  # e[4] contains the actual tweet\n",
    "        x_batch = [self.line_to_indices(e, self.max_seq_len) for e in x_batch]\n",
    "        x_batch = torch.LongTensor(x_batch)\n",
    "        return x_batch\n",
    "\n",
    "    def process_y(self, raw_y_batch):\n",
    "        y_batch = [int(e) for e in raw_y_batch]\n",
    "        return y_batch\n",
    "\n",
    "    def split_line(self, line):\n",
    "        \"\"\"\n",
    "        Split given line/phrase (str) into list of words (List[str])\n",
    "        \"\"\"\n",
    "        return re.findall(r\"[\\w']+|[.,!?;]\", line)\n",
    "\n",
    "    def flatten_list(self, nested_list):\n",
    "        return list(itertools.chain.from_iterable(nested_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b3cb603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load the train and test datasets.\n",
    "train_dataset = Sent140Dataset(\n",
    "    data_root=TRAIN_DATA,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    ")\n",
    "test_dataset = Sent140Dataset(\n",
    "    data_root=TEST_DATA,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25ae73bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating FL User: 2493user [00:00, 26797.06user/s]\n",
      "Creating FL User: 2493user [00:00, 32796.88user/s]\n",
      "Creating FL User: 2493user [00:00, 33634.41user/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Clients in total: 2493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from flsim.utils.example_utils import LEAFDataLoader, LEAFDataProvider\n",
    "\n",
    "# 3. Batchify training, eval, and test data. Note that train_dataset is already sharded.\n",
    "dataloader = LEAFDataLoader(\n",
    "    train_dataset,\n",
    "    test_dataset,\n",
    "    test_dataset,\n",
    "    batch_size=LOCAL_BATCH_SIZE,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "# 4. Wrap the data loader with a data provider.\n",
    "data_provider = LEAFDataProvider(dataloader)\n",
    "print(f\"\\nClients in total: {data_provider.num_users()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26b3e456",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class CharLSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes,\n",
    "        n_hidden,\n",
    "        num_embeddings,\n",
    "        embedding_dim,\n",
    "        max_seq_len,\n",
    "        dropout_rate,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.n_hidden = n_hidden\n",
    "        self.num_classes = num_classes\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.num_embeddings = num_embeddings\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=self.num_embeddings, embedding_dim=embedding_dim\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=self.n_hidden,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=self.dropout_rate,\n",
    "        )\n",
    "        self.fc = nn.Linear(self.n_hidden, self.num_classes)\n",
    "        self.dropout = nn.Dropout(p=self.dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_lens = torch.sum(x != (self.num_embeddings - 1), 1) - 1\n",
    "        x = self.embedding(x)  # [B, S] -> [B, S, E]\n",
    "        out, _ = self.lstm(x)  # [B, S, E] -> [B, S, H]\n",
    "        out = out[torch.arange(out.size(0)), seq_lens]\n",
    "        out = self.fc(self.dropout(out))  # [B, S, H] -> # [B, S, C]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af8e76d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flsim.utils.example_utils import FLModel\n",
    "import flsim.configs\n",
    "from flsim.utils.config_utils import fl_config_from_json\n",
    "from flsim.interfaces.metrics_reporter import Channel\n",
    "from flsim.utils.example_utils import MetricsReporter\n",
    "from omegaconf import OmegaConf\n",
    "import math\n",
    "from hydra.utils import instantiate\n",
    "import copy\n",
    "\n",
    "\n",
    "'''\n",
    "TODO:\n",
    "- Put all this into a function\n",
    "- Another dimension for momentum\n",
    "- Properly adjust resource parameters for a good run that doesn't take too long, and get results\n",
    "    - 'resources' in  FLSim_Adam_objective()\n",
    "        - Currently treating resources and epochs as equal, communication_rounds will need to be calculated\n",
    "    - USERS_PER_ROUND (users_per_round in json_config)\n",
    "'''\n",
    "\n",
    "dimensions = [Real(1e-4, 1e-1, name=\"lr\")]\n",
    "\n",
    "# Create a metric reporter.\n",
    "metrics_reporter = MetricsReporter([Channel.TENSORBOARD, Channel.STDOUT])\n",
    "\n",
    "USERS_PER_ROUND = 100 # needs adjusting\n",
    "\n",
    "def FLSim_Adam_objective(resources, checkpoint, **hyperparameters):\n",
    "    '''\n",
    "    resources: the number of communication rounds the FLSim trainer is allowed\n",
    "    checkpoint: should be a way to load model progress\n",
    "    hyperparameters: are sourced from the named dimensions (see above)\n",
    "    '''\n",
    "    lr = hyperparameters[\"lr\"]\n",
    "#     momentum = hyperparameters[\"momentum\"]\n",
    "#     epochs = math.floor(resources / math.ceil(data_provider.num_users()/USERS_PER_ROUND) )\n",
    "    epochs = resources\n",
    "#     assert epochs>=1\n",
    "    \n",
    "    model = CharLSTM(\n",
    "        num_classes=train_dataset.num_classes,\n",
    "        n_hidden=100,\n",
    "        num_embeddings=train_dataset.num_letters + 1,\n",
    "        embedding_dim=100,\n",
    "        max_seq_len=MAX_SEQ_LEN,\n",
    "        dropout_rate=0.1,\n",
    "    )\n",
    "    if checkpoint is not None:\n",
    "        model.load_state_dict(checkpoint)       \n",
    "    \n",
    "    # 2. Choose where the model will be allocated.\n",
    "    cuda_enabled = torch.cuda.is_available() and USE_CUDA\n",
    "    device = torch.device(f\"cuda:{0}\" if cuda_enabled else \"cpu\")\n",
    "    \n",
    "    # 3. Wrap the model with FLModel.\n",
    "    global_model = FLModel(model, device) # model gets updated as global_model is trained\n",
    "    assert(global_model.fl_get_module() == model)\n",
    "    \n",
    "    # 4. Move the model to GPU and enable CUDA if desired.\n",
    "    if cuda_enabled:\n",
    "        global_model.fl_cuda()\n",
    "        \n",
    "    json_config = {\n",
    "        \"trainer\": {\n",
    "            \"_base_\": \"base_sync_trainer\",\n",
    "            \"server\": {\n",
    "                \"_base_\": \"base_sync_server\",\n",
    "                \"server_optimizer\": {\n",
    "                    # there are different types of server optimizers\n",
    "                    # fed avg with lr requires a learning rate, whereas e.g. fed_avg doesn't\n",
    "                      # \"_base_\": \"base_fed_avg_with_lr\",\n",
    "                    # server's learning rate\n",
    "                      # \"lr\": 0.7,\n",
    "                    # server's global momentum\n",
    "                      # \"momentum\": 0.9\n",
    "\n",
    "                    # Federated ADAM (with weight decay)\n",
    "                    # Server Defaults:  \n",
    "                      # lr: float = 0.001\n",
    "                      # weight_decay: float = 0.00001\n",
    "                      # beta1: float = 0.9\n",
    "                      # beta2: float = 0.999\n",
    "                      # eps: float = 1e-8\n",
    "                    \"_base_\": \"base_fed_adam\",\n",
    "                    \"lr\": lr\n",
    "                },\n",
    "                # aggregate client models into a single model by taking their weighted sum\n",
    "                \"aggregation_type\": \"WEIGHTED_AVERAGE\",\n",
    "                # type of user selection sampling\n",
    "                \"active_user_selector\": {\n",
    "                    \"_base_\": \"base_uniformly_random_active_user_selector\"\n",
    "                }\n",
    "            },\n",
    "            \"client\": {\n",
    "                # number of client's local epochs\n",
    "                # \"epochs\": 1,  <--- old value in example\n",
    "                \"epochs\": 10,\n",
    "                \"optimizer\": {\n",
    "                    # client's optimizer\n",
    "                    \"_base_\": \"base_optimizer_sgd\",\n",
    "                    # client's local learning rate\n",
    "                    # \"lr\": 1,\n",
    "                    \"lr\": 0.1,\n",
    "                    # client's local momentum\n",
    "                    \"momentum\": 0\n",
    "                }\n",
    "            },\n",
    "            # number of users per round for aggregation\n",
    "            \"users_per_round\": USERS_PER_ROUND,\n",
    "            # total number of global epochs\n",
    "            # total #rounds = ceil(total_users / users_per_round) * epochs <---- THIS IS THE MAIN COMMUNICATION COST METRIC\n",
    "            #   total_users = ~2500, data_provider.num_users()\n",
    "            # \"epochs\": 1,\n",
    "            \"epochs\": epochs,\n",
    "            # frequency of reporting train metrics\n",
    "            \"train_metrics_reported_per_epoch\": 1,\n",
    "            # keep the trained model always (as opposed to only when it\n",
    "            # performs better than the previous model on eval)\n",
    "            \"always_keep_trained_model\": False,\n",
    "            # frequency of evaluation per epoch\n",
    "            \"eval_epoch_frequency\": 1,\n",
    "            \"do_eval\": True,\n",
    "            # should we report train metrics after global aggregation\n",
    "            \"report_train_metrics_after_aggregation\": True\n",
    "        }\n",
    "    }\n",
    "\n",
    "    cfg = fl_config_from_json(json_config)\n",
    "#     if VERBOSE: print(OmegaConf.to_yaml(cfg))\n",
    "\n",
    "    trainer = instantiate(cfg.trainer, model=global_model, cuda_enabled=cuda_enabled)   \n",
    "    # Launch FL training.\n",
    "    final_model, eval_score = trainer.train( # eval_score should have a value, but it's returning None?\n",
    "        data_provider=data_provider,\n",
    "        metric_reporter=metrics_reporter,\n",
    "        num_total_users=data_provider.num_users(),\n",
    "        distributed_world_size=1,\n",
    "    )\n",
    "    \n",
    "    acc = trainer.test(\n",
    "                data_iter=data_provider.test_data(),\n",
    "                metric_reporter=MetricsReporter([Channel.STDOUT]),\n",
    "            )[\"Accuracy\"]\n",
    "    \n",
    "    ret_checkpt = copy.deepcopy(model.state_dict())\n",
    "    return (-acc),  ret_checkpt # -acc since the hyperband functions aims to minimize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae85ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies, hps = hyperband(objective=FLSim_Adam_objective, \n",
    "                            dimensions=dimensions,\n",
    "                            downsample=2,\n",
    "                            max_resources_per_model=2**5) # this parameter needs adjusting\n",
    "for acc, hp in zip(accuracies, hps):\n",
    "    print(acc, hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbfbcae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
